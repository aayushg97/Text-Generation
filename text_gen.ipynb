{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "709c5453",
      "metadata": {
        "id": "709c5453"
      },
      "source": [
        "## <font color='blue'>Text generation using an n-gram model</font>\n",
        "\n",
        "This project trains a language model on Shakespeare and uses it to generate text. The model will be bare-bones but has the same underlying ideas as much more advanced models that are in wide use. Simply put, language models assign a likelihood (probability) to a given sentence. For example, the sentence \"dog is barking\" has a higher likelihood than \"bark dog is\", which is grammatically incoherent.\n",
        "\n",
        "### <font color='blue'>Bigram and trigram models</font>\n",
        "\n",
        "Suppose we have a sentence composed of words (or tokens) $x_1, x_2, ..., x_n$. We want to be able to compute the probability of this sequence, $P(x_1, x_2, ... x_n)$. The distribution can always be factored as\n",
        "\n",
        "$$  P(x_1, x_2, ... x_n)  =  P(x_1) \\prod_{i=2}^{n}P(x_i|x_{i-1}, x_{i-2}, ..., x_1) .$$\n",
        "\n",
        "However, these conditional probabilities are hard to model for even medium-sized vocabularies. One simplification is to make a <em>first-order Markov assumption</em>, which says that the probability of a word given all previous words is equal to the probability of the word given just the one word before it, that is,\n",
        "\n",
        "$$ P(x_i | x_1, x_2, ..., x_{i-1}) = P(x_i | x_{i-1}). $$\n",
        "\n",
        "This allows us to factor\n",
        "\n",
        "$$ P(x_1, x_2, ... x_n)  = \\prod_{i=1}^{n}P(x_i|x_{i-1}) $$\n",
        "\n",
        "where $x_0$ is a special \"START\" symbol.\n",
        "\n",
        "The formulation above is called a <em>bigram</em> language model, since it is based on pairs of consecutive words. If the context is expanded to include the two previous words, we get a <em>trigram</em> model,\n",
        "    \n",
        "$$ P(x_1, x_2, ... x_n)  = \\prod_{i=1}^{n}P(x_i|x_{i-2}, x_{i-1}) $$\n",
        "\n",
        "where $x_{-1}, x_0$ are special start symbols.\n",
        "\n",
        "In the same way, an <em>n-gram</em> model can be defined by expanding the context to include the $n-1$ previous words.\n",
        "\n",
        "### <font color='blue'>Learning an $n$-gram model</font>\n",
        "\n",
        "Here, <em>maximum-likelihood estimation</em> will be used to learn an $n$-gram model. For this, we simply need to count the number of occurrences of each $n$-gram and $n-1$ gram in the corpus.\n",
        "\n",
        "For instance, to train a trigram model, let $c(u,v,w)$ be the number of times that trigram $(u,v,w)$ is seen in the training corpus and let $c(u,v)$ be the number of times that bigram $(u,v)$ is seen. The maximum likelihood estimate of $P(w|u,v)$ is then\n",
        "$$ P(w|u,v) = \\frac{c(u,v,w)}{c(u,v)} .$$\n",
        "Of course, we might want to smooth this using Laplace smoothing or something similar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82556bde",
      "metadata": {
        "id": "82556bde"
      },
      "source": [
        "### <font color='blue'>Reading in the data</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47320ab7",
      "metadata": {
        "id": "47320ab7"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33029d64",
      "metadata": {
        "id": "33029d64"
      },
      "source": [
        "The provided text file `shakespeare.txt` contains a selection of Shakepeare's sonnets and plays. It also contains stage directions, copyright notices, and various other things that we won't bother removing. We will read in a list of all sentences in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b3fff5",
      "metadata": {
        "id": "15b3fff5"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "with open(\"shakespeare.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    text = f.read()\n",
        "    text = text.split(\".\")\n",
        "    for sentence in text:\n",
        "        sentence += \".\"\n",
        "        sentences.append(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4761bba8",
      "metadata": {
        "id": "4761bba8"
      },
      "source": [
        "Here is an example of one of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dc42cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "34dc42cc",
        "outputId": "48b82e83-10ae-49f2-c2dd-b6e8dfbefd01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n                     82\\n  I grant thou wert not married to my muse,\\n  And therefore mayst without attaint o'erlook\\n  The dedicated words which writers use\\n  Of their fair subject, blessing every book.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "sentences[200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d429398c",
      "metadata": {
        "id": "d429398c"
      },
      "source": [
        "Let's set aside the very first sentence (we'll use it later). We'll extract all words from the remainder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07886413",
      "metadata": {
        "id": "07886413"
      },
      "outputs": [],
      "source": [
        "test_play = sentences[0]\n",
        "sentences = sentences[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81bae7d",
      "metadata": {
        "id": "e81bae7d"
      },
      "outputs": [],
      "source": [
        "words = set()\n",
        "for sent in sentences:\n",
        "    for word in sent.split():\n",
        "        words.add(word)\n",
        "vocab = list(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dfb53f3",
      "metadata": {
        "id": "4dfb53f3"
      },
      "source": [
        "### <font color='blue'>Class: NGramModel</font>\n",
        "\n",
        "This class holds an n-gram model.\n",
        "\n",
        "<font color='magenta'>`generate_n_grams`:</font> given a sequence (list) of tokens, it returns all of its ngrams. For example, for `tokens = ['Unsupervised',  'Learning', 'is', 'fun', '.']`, the trigrams would be `[(['START', 'START'], 'Unsupervised'), (['START', 'Unsupervised'], 'Learning') ...]`.\n",
        "\n",
        "<font color='magenta'>`get_prob`:</font> takes in a list of context tokens and a target token and returns the probability of the target given the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb239f2",
      "metadata": {
        "id": "2fb239f2"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class NGramModel(object):\n",
        "    def __init__(self, n: int) -> None:\n",
        "        self.n = n\n",
        "        self.n_grams = dict() # Stores unique n-grams\n",
        "        self.context_count = dict() # Stores count of contexts\n",
        "        self.ngram_count = dict() # Stores count of ngrams: (context, token)\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        # Tokenize the given sentence 'text'\n",
        "        # Treat punctuation as a separate token.\n",
        "        # Add space before punctuation.\n",
        "        # Split using spaces.\n",
        "\n",
        "        for ch in string.punctuation:\n",
        "            text = text.replace(ch, \" \" + ch)\n",
        "        tokens = text.strip().split()\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def generate_n_grams(self, tokens: List[str]) -> List[Tuple[List[str], str]]:\n",
        "        # Generate all n-grams from the given list of tokens\n",
        "        # Insert (n-1) <START> tokens before each sentence\n",
        "        tokens = (self.n-1)*[\"<START>\"] + tokens\n",
        "        n_grams = list()\n",
        "\n",
        "        \"\"\"\n",
        "        n_grams is a list where each element is\n",
        "        ([n-1 context tokens], token)\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(0, len(tokens)-self.n+1):\n",
        "            n_grams.append((tokens[i:i+self.n-1], tokens[i+self.n-1]))\n",
        "\n",
        "        return n_grams\n",
        "\n",
        "    def fit(self, text: str) -> None:\n",
        "        # Takes a sentence 'text'\n",
        "        # Generates all n-grams in the sentence\n",
        "        # Then updates counts\n",
        "        new_n_grams = self.generate_n_grams(self.tokenize(text))\n",
        "        for context, target in new_n_grams:\n",
        "            # Add context to context dict and store count\n",
        "            if tuple(context) in self.context_count:\n",
        "                self.context_count[tuple(context)] += 1.0\n",
        "            else:\n",
        "                self.context_count[tuple(context)] = 1.0\n",
        "\n",
        "            # Save unique n_grams.\n",
        "            # This part is used for generation only.\n",
        "            if tuple(context) in self.n_grams:\n",
        "                if target not in self.n_grams[tuple(context)]:\n",
        "                    self.n_grams[tuple(context)].append(target)\n",
        "            else:\n",
        "                self.n_grams[tuple(context)] = [target]\n",
        "\n",
        "            # Store n_gram counts\n",
        "            new_n_gram = (tuple(context), target)\n",
        "            if new_n_gram in self.ngram_count:\n",
        "                self.ngram_count[new_n_gram] += 1.0\n",
        "            else:\n",
        "                self.ngram_count[new_n_gram] = 1.0\n",
        "\n",
        "\n",
        "\n",
        "    def get_prob(self, context: List[str], target: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the probability of the target token\n",
        "        given the context.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.ngram_count[(tuple(context), target)]/self.context_count[tuple(context)]\n",
        "\n",
        "\n",
        "    def predict_token(self, context: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Predict the next token given context.\n",
        "        A slight randomness ensures we generate a diverse token\n",
        "        with the same context.\n",
        "        \"\"\"\n",
        "        r = random.random()\n",
        "        # store the probability of each token.\n",
        "        token_probs = dict()\n",
        "\n",
        "        try:\n",
        "            tokens_of_interest = self.n_grams[tuple(context)]\n",
        "            for token in tokens_of_interest:\n",
        "                token_probs[token] = self.get_prob(context, token)\n",
        "        except KeyError: # similar to Laplace smoothing; returns a random word from the vocab.\n",
        "            ridx = random.randint(0, len(words))\n",
        "            return vocab[ridx]\n",
        "\n",
        "\n",
        "        sum = 0.0\n",
        "        for key in sorted(token_probs):\n",
        "            sum += token_probs[key]\n",
        "            # When the probability sum is > random number\n",
        "            # return the current token.\n",
        "            if sum > r:\n",
        "                return key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a226b5e9",
      "metadata": {
        "id": "a226b5e9"
      },
      "source": [
        "### <font color='blue'>Routines for fitting data and generating text</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30c2868",
      "metadata": {
        "id": "f30c2868"
      },
      "source": [
        "`create_and_fit_model` defines an n-gram model and fits it to data. Its parameters are `n` (the order of the model) and `sentences` (collection of sentences).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a183de4",
      "metadata": {
        "id": "3a183de4"
      },
      "outputs": [],
      "source": [
        "def create_and_fit_model(n, sentences):\n",
        "    \"\"\"\n",
        "    This is the key function that defines and fits an n-gram model.\n",
        "    It takes in n and a list of sentences.\n",
        "    It creates an n-gram model and then calls the `fit` method on\n",
        "    one sentence at a time to generate counts.\n",
        "    \"\"\"\n",
        "    model = NGramModel(n)\n",
        "    for sent in sentences:\n",
        "        model.fit(sent)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8a5bb0",
      "metadata": {
        "id": "7d8a5bb0"
      },
      "source": [
        "`generate_text` uses an n-gram model to generate text, starting from a prompt (which might be empty). It takes as input the `model`, the number of words to generate (`n_outs`) and the `prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b51c8b9",
      "metadata": {
        "id": "0b51c8b9"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: NGramModel, n_outs: int, prompt=None) -> str:\n",
        "    \"\"\"\n",
        "    Generates n_outs words using the trained\n",
        "    ngram model.\n",
        "    \"\"\"\n",
        "    n = model.n\n",
        "    # All sentence are initialized with the <START> token\n",
        "\n",
        "    if prompt is not None:\n",
        "        prompt_tokens = model.tokenize(prompt)\n",
        "        context_queue = prompt_tokens[-(n-1):]\n",
        "    else:\n",
        "        context_queue = (n-1) * [\"<START>\"]\n",
        "    result = list()\n",
        "\n",
        "    for _ in range(n_outs):\n",
        "        pred_token = model.predict_token(context_queue)\n",
        "        result.append(pred_token)\n",
        "\n",
        "        context_queue.pop(0)\n",
        "\n",
        "        if pred_token == \".\":\n",
        "            # If sentence done. Start a new sentence.\n",
        "            context_queue = (n-1) * [\"<START>\"]\n",
        "        else:\n",
        "            context_queue.append(pred_token)\n",
        "\n",
        "    return \" \".join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc401f79",
      "metadata": {
        "id": "bc401f79"
      },
      "outputs": [],
      "source": [
        "def print_generated_text(model):\n",
        "    \"\"\"\n",
        "    Prints a 100-word blurb from the provided model.\n",
        "    \"\"\"\n",
        "    num_gen_words = 100\n",
        "    print(f'{\"=\"*50}\\nGenerated text:')\n",
        "    print(\"\\n\")\n",
        "    print(generate_text(model, num_gen_words))\n",
        "    print(f'{\"=\"*50}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb221db",
      "metadata": {
        "id": "beb221db"
      },
      "source": [
        "### <font color='blue'>Experimenting with text generation</font>\n",
        "\n",
        "As an experiment, `create_and_fit` is used to create n-gram models for n=2 and n=3 which are fit to the provided text from Shakespeare. Then `print_generated_text` is used to generate text from each of the two models. Finally the two resulting texts are reported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbece21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fbece21",
        "outputId": "45a62602-0e26-461b-8083-763fdbad6ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Generated text:\n",
            "\n",
            "\n",
            "Go with flowing tides and mart , Catesby , and , you 'll have again , madam ; we will keep 't , Hews down His sword impress . JOHN . These good lord , the beaten ? Look 'd by their congeal again in state in this was done any he ? A little prating coxcomb ? Why should be more To withdraw yourself ; I come -O Jove would you stay , Like perspectives which nature reigned , For he might please . AGRIPPA , that ? MESSENGER . Here 's mistress , which I could control thee\n",
            "==================================================\n",
            "==================================================\n",
            "Generated text:\n",
            "\n",
            "\n",
            "VALENTINE . Exeunt SCENE VIII . MESSENGER . TROILUS . Another way so happily ; the weight of a greater falseness ; Which were inshell 'd when Marcius stood for his offence . Then all alone beweep my outcast state , And deeper than e 'er my haps , my lord , The imminent death of the knee Where thrift may follow , Stephano ! O Imogen ! IMOGEN . DECIUS . We should be visited . Pardon , master ; red as Mars his idiot ! Do you hear the suit Of Count Orsino 's embassy . For God\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "bigram_model = create_and_fit_model(2, sentences)\n",
        "print_generated_text(bigram_model)\n",
        "trigram_model = create_and_fit_model(3, sentences)\n",
        "print_generated_text(trigram_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6cf7ec",
      "metadata": {
        "id": "dc6cf7ec"
      },
      "source": [
        "### <font color='blue'>Computing likelihoods</font>\n",
        "\n",
        "In this part, the log-likelihood of a sentence is calculated using the models that we just created. This gives a concrete indication of why incorporating more context is helpful. For a given sentence $s$ with $n$ tokens $(x_1, \\ldots, x_n)$, the log-likelihood is defined as\n",
        "$$ L(s) = \\sum_{i=1}^{n}\\log p(x_i|\\mbox{context}) $$\n",
        "For a unigram model, this maximum-likelihood estimates of $p(x_i)$ would simply be\n",
        "$$ p(x_i) = \\frac{c(x_i)}{N}$$\n",
        "where N is the total number of words in the dataset and $c(x_i)$ is the number of occurrences of $x_i$. Similarly, for a bigram model, we have\n",
        "$$ p(x_i|\\mbox{context}) = \\frac{c(x_{i-1}, x_i)}{c(x_{i-1})} .$$\n",
        "In the code, these probabilities are provided by `get_prob` in `NGramModel`.\n",
        "\n",
        "The following function that takes an `NGramModel` and a sentence as input and returns the log-likelihood of that sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e5d9e1",
      "metadata": {
        "id": "a1e5d9e1"
      },
      "outputs": [],
      "source": [
        "def model_ll(model, text):\n",
        "    \"\"\"\n",
        "    Takes in an n-gram model and a sample text.\n",
        "    Returns the log-likelihood of the text under the given model.\n",
        "    \"\"\"\n",
        "    import math\n",
        "    ll = 0\n",
        "\n",
        "    # 1. Tokenize the text.\n",
        "    tokens = model.tokenize(text)\n",
        "\n",
        "    # 2. Generate ngrams for the tokens.\n",
        "    ngrams = model.generate_n_grams(tokens)\n",
        "\n",
        "    # 3. Loop through the ngrams, calculate log prob for each ngram and update ll.\n",
        "    for context, target in ngrams:\n",
        "        ll += math.log(model.get_prob(context, target))\n",
        "\n",
        "    return ll"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa914c65",
      "metadata": {
        "id": "fa914c65"
      },
      "source": [
        "With the sentence \"And on a love-book pray for my success?\" as input. The log-likelihood of this sentence under a unigram model and a bigram model is calculated and reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0fc28b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e0fc28b",
        "outputId": "28f050e0-def3-42b1-c68e-9b9a73344470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log-likelihood under unigram model :-  -65.86143429059021\n",
            "Log-likelihood under bigram model :-  -45.879305709530776\n"
          ]
        }
      ],
      "source": [
        "text = \"And on a love-book pray for my success?\"\n",
        "\n",
        "unigram_model = create_and_fit_model(1, sentences)\n",
        "print(\"Log-likelihood under unigram model :- \", model_ll(unigram_model, text))\n",
        "print(\"Log-likelihood under bigram model :- \", model_ll(bigram_model, text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c1804fd",
      "metadata": {
        "id": "1c1804fd"
      },
      "source": [
        "### <font color='blue'>Text completion</font>\n",
        "\n",
        "Given a prompt from `test_play` (the held out part of the first sonnet), the following code generates 20 words of text (using the function `generate_text`) from the bigram and the trigram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f9e6e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69f9e6e2",
        "outputId": "c47f03c9-0f1d-42bb-d4e2-f0ccdb965217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE SONNETS\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n"
          ]
        }
      ],
      "source": [
        "print(test_play)\n",
        "prompt = \"From fairest creatures we desire increase\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d228ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "66d228ab",
        "outputId": "b36999e6-de3c-498f-bfcd-498b719a5506"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\". Another part . Yet in the commons KING HENRY . NORTHUMBERLAND . Exeunt . ' Was beastly dumb and\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "generate_text(bigram_model, 20, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c94d655",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9c94d655",
        "outputId": "bdf43e7c-b968-4af6-fee5-ba542d2d7662"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Naiads, testament- hypocrite! unseeing reward, cheeks! (sings) perdu!- that's-when? Samp. tallest! missing. native, basin uses. weakness. perform, incurable. braggarts Mall's\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "generate_text(trigram_model, 20, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09317587",
      "metadata": {
        "id": "09317587"
      },
      "source": [
        "There are various tweaks that would improve this model: for instance, careful <em>smoothing</em> and using longer-range dependencies (via variable-length Markov models such as <em>probabilistic suffix trees</em>). The next big boost in performance would come from replacing tabular estimates of conditional probabilities $P(x_i|x_1, ...., x_{i-1})$ by <em>recurrent neural nets</em>."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "189px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}